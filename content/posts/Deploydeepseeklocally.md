+++
title = '🚀 Hardware Requirements Analysis for Local Deployment of Deepseek'
date = 2025-01-29T18:31:06+08:00
draft = false
+++

# 🖥️ Hardware Requirements Analysis for Local Deployment of Deepseek-R1

Deepseek 的杰出表现给很多人留下了深刻的印象。其中 **Deepseek-R1** 和 **Deepseek-V3** 都是非常优秀的模型 🎯，但是二者的侧重点不同：
- **DS-R1** 更加侧重于 **数学、代码和推理功能** 💡。
- **DS-V3** 专注于 **自然语言处理、知识问答、内容创作** ✍️ 等通用任务。

事实上，DS 之前就同时推出了 2 款模型，分别侧重于一般对话和代码生成。

鉴于 DS 模型的优秀表现，**如果能在本地部署，并且能够以较快的速度推理，无疑会极大地提高用户的使用效率和稳定性 🚀**。这里要额外说明 **本地部署的含义**：
- **模型的计算推理完全基于本地的硬件** 🏠，只要你的电脑通电，不管是否联网都能完成推理。

为此， 笔者基于公开数据简单推理，如果在 **本地部署** 这两个模型所需要的 **硬件设备 🛠️**。好消息是 **两个模型的参数规模基本一样，总参数都是 671B，激活参数 37B** 📊。

📢 **声明**：笔者能力有限，以下内容如有错误，还请读者及时沟通反馈 🤝。

---

## **🖥️ Motherboard（主板）**
本地部署模型有两种解决方案：
1. **CPU + 内存 🏗️**
2. **GPU + 显存 🎮**

这里我们采用 **方案一（CPU+内存）**，那么 **内存带宽和内存容量** 就是两个非常重要的因素 📏📦。因为在推理过程中：
- **模型权重和中间数据的存储都在内存中**，计算过程则在 CPU 中完成 🧠。
- 由于 **LLM 模型的权重非常大**，我们需要 **足够的内存插槽** 才能应对。

✅ **选择：技嘉 MZ73-LM0 PCIE5.0 主板**，💰 **CNY 7799**。
- 该主板提供 **24 个内存插槽 🏆** 和 **12 个内存通道 🔥**，可以提供极高的带宽和扩展性。

---

## **⚙️ CPU（处理器）**
考虑到 **LLM 推理对 CPU 计算能力的需求并不高** 📉，因此 **CPU 预算可以适当降低 💰**。

✅ **选择：AMD EPYC 9655（96 核心）**，💰 **CNY 49,000**。
- 由于 **主板支持双路 CPU**，因此 **总预算 = CNY 98,000** 💵💵。

---

## **🛠️ 内存（RAM）**
💡 **内存是 LLM 推理过程中最关键的部分之一**。

### **📊 需求计算**
- 假设采用 **低精度 FP8**，我们可以通过 [计算器](https://gpumemoryestimator.williamswang.win/) 估算内存需求：
  - **671B 参数（FP8） ≈ 624.92GB**。
  - **考虑中间激活状态和其他开销**，我们需要采购 **24 条 32GB DDR5 内存** 🏗️。

✅ **选择：美商海盗船 64GB（32GB ×2）DDR5-6000 内存**：
- **12 组，每组 CNY 1449**。
- **总预算 = CNY 17,388 💵**。

---

## **🚀 内存带宽（Memory Bandwidth）**
在上述配置下，**主板提供 12 通道 DDR5（12 Memory Channels），24 个 DIMM 插槽（每通道 2 个 DIMM）** 🔥。

**理论带宽计算**：
- **DDR5-6000** 理论带宽：
  - **单通道带宽** = 48 GB/s。
  - **12 通道总带宽** = **576 GB/s** 🚀。
- **考虑降频的可能性**：
  - **DDR5-5600** → **537.6 × 0.85 = 457 GB/s** 🧐。
  - **DDR5-5200** → **499.2 × 0.85 = 424 GB/s** 🧐。

⚠️ **注意**：由于 **2DPC 配置可能会导致内存降频** 📉，实际带宽可能比理论值略低。

---

## **💼 机箱（Case）**
💡 **正常家用机箱不一定能容纳该主板**，建议 **定制机柜 🏗️**。

✅ **预算：CNY 400** 💰。

---

## **⚡ 电源（PSU）**
考虑到 **主要功耗来自 2 颗 CPU**（每颗 400W），总功耗约 **800W** 🔋。

✅ **选择：SEASONIC 海韵 1200W 金牌 VERTEX 电源**：
- 预算：**CNY 1999** ⚡。

---

## **💾 硬盘（Storage）**
LLM **对硬盘要求不高**，因为 **所有模型权重存储在内存中 🛠️**。

✅ **选择：西部数据（WD）NVMe SSD**：
- 预算：**CNY 859** 💾。

---

## **❄️ 散热设备（Cooling System）**
考虑到 **LLM 运行过程的高负载**，建议搭配 **强力散热设备 🌀**。

✅ **预算：CNY 2000** 💰。

---

## **📢 Conclusion（总结）**
到目前为止，**总预算 = 💰 CNY 128,455** 🎯，价格远低于 **NVIDIA 数据中心 GPU 方案 💸**。

⚠️ **性能预估**
- **CPU-only 方案** 的推理速度可能 **不超过 10 tokens/s** ⏳，但仍然可以流畅体验 Deepseek-R1 🚀。

✅ **下一步**
- 你可以从 **Hugging Face 下载模型** 📥 并部署在你的计算机上，享受最新的 **DS-V3 或 DS-R1** 🔥！

---

🎉 **Happy Hacking & Enjoy Your Local AI Setup! 🏆**